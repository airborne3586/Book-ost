import streamlit as st
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

import matplotlib.pyplot as plt
import seaborn as sns
from itertools import islice

import requests
import json

from sklearn.metrics.pairwise import cosine_similarity

from selenium import webdriver
from selenium.webdriver.common.by import By

import time

from googletrans import Translator

import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.corpus import wordnet
from nltk.stem import PorterStemmer, WordNetLemmatizer
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
nltk.download('stopwords')

from sklearn.feature_extraction.text import TfidfVectorizer
import joblib

data = pd.read_excel('final_data.xlsx',index_col=0)

st.header('1ï¸âƒ£ ì±… ì œëª©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.')

book_title =  st.text_input(label = 'ì˜ˆì‹œ) ë‚ ì”¨ê°€ ì¢‹ìœ¼ë©´ ì°¾ì•„ê°€ê² ì–´ìš”',value="",key='text')

def reset():
    st.session_state.text = ""

reset = st.button('Reset',on_click=reset)
if not book_title:
        con = st.container()
        con.caption('Result')
        con.error('ì±… ì œëª©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.',icon="âš ï¸")
        st.stop()


rest_api_key = "41d651c93152d5ec054dc828cacfa671"
url = "https://dapi.kakao.com/v3/search/book"
header = {"authorization": "KakaoAK "+rest_api_key}
querynum = {"query": book_title}

try:
    response = requests.get(url, headers=header, params = querynum)
    content = response.text
    ì±…ì •ë³´ = json.loads(content)['documents'][0]
except:
    con = st.container()
    con.caption('Result')
    con.error('ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì±…ì…ë‹ˆë‹¤. ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”.',icon="ğŸš¨")
    st.stop()

book = pd.DataFrame({'title': ì±…ì •ë³´['title'],
              'isbn': ì±…ì •ë³´['isbn'],
              'authors': ì±…ì •ë³´['authors'],
              'publisher': ì±…ì •ë³´['publisher']})

target_url = ì±…ì •ë³´['url']


# ì˜µì…˜ ìƒì„±
options = webdriver.ChromeOptions()
# ì°½ ìˆ¨ê¸°ëŠ” ì˜µì…˜ ì¶”ê°€
options.add_argument("headless")

driver = webdriver.Chrome(options=options)
driver.get(target_url)

img= driver.find_element(By.XPATH, '//*[@id="tabContent"]/div[1]/div[1]/div[1]/span/img')
img_src = img.get_attribute('src')

col1, col2 = st.columns([1,2])
with col1:
    st.image(img_src,width=150)
with col2:
    title = book['title'][0]
    author = book['authors'][0]
    publisher = book['publisher'][0]
    
    st.caption('ì œëª© : '+ title)
    st.caption('ì €ì : '+ author)
    st.caption('ì¶œì‚°ì‚¬ : '+publisher)

st.title('')
text = '<'+title +'>ì— ëŒ€í•œ ì •ë³´ë¥¼ ëª¨ìœ¼ê³  ìˆëŠ” ì¤‘ì…ë‹ˆë‹¤.'
my_bar = st.progress(0, text=text)
time.sleep(5)
my_bar.progress(5, text='ã€°ï¸5%ã€°ï¸')

try :
    botton = driver.find_element(By.XPATH, '//*[@id="tabContent"]/div[1]/div[2]/div[3]/a')
    botton.click()
except :
    pass
ì±…ì†Œê°œ = driver.find_element(By.XPATH, '//*[@id="tabContent"]/div[1]/div[2]/p')

time.sleep(3)
my_bar.progress(10, text='ã€°ï¸10%ã€°ï¸')
try :
    botton = driver.find_element(By.XPATH, '//*[@id="tabContent"]/div[1]/div[5]/div[3]/a')
    botton.click()
except :
    pass
ì±…ì†ìœ¼ë¡œ = driver.find_element(By.XPATH, '//*[@id="tabContent"]/div[1]/div[5]/p')

time.sleep(3)
my_bar.progress(20, text='ã€°ï¸20%ã€°ï¸')
try :
    botton = driver.find_element(By.XPATH, '//*[@id="tabContent"]/div[1]/div[6]/div[3]/a')
    botton.click()
except :
    pass
ì„œí‰ = driver.find_element(By.XPATH, '//*[@id="tabContent"]/div[1]/div[6]/p')


book['ì±…ì†Œê°œ'] = ì±…ì†Œê°œ.text
book['ì±…ì†ìœ¼ë¡œ'] = ì±…ì†ìœ¼ë¡œ.text
book['ì„œí‰'] = ì„œí‰.text
driver.close()

time.sleep(1)
my_bar.progress(30, text='ã€°ï¸30%ã€°ï¸')


#ì˜ì–´ ë¶ˆìš©ì–´ ì‚¬ì „
stops = set(stopwords.words('english'))

def hapus_url(text):
    mention_pattern = r'@[\w]+'
    cleaned_text = re.sub(mention_pattern, '', text)
    return re.sub(r'http\S+','', cleaned_text)

#íŠ¹ìˆ˜ë¬¸ì ì œê±°
#ì˜ì–´ ëŒ€ì†Œë¬¸ì, ìˆ«ì, ê³µë°±ë¬¸ì(ìŠ¤í˜ì´ìŠ¤, íƒ­, ì¤„ë°”ê¿ˆ ë“±) ì•„ë‹Œ ë¬¸ìë“¤ ì œê±°
def remove_special_characters(text, remove_digits=True):
    text=re.sub(r'[^a-zA-Z0-9\s]', '', text)
    return text


#ë¶ˆìš©ì–´ ì œê±°
def delete_stops(text):
    text = text.lower().split()
    text = ' '.join([word for word in text if word not in stops])
    return text
   
    
#í’ˆì‚¬ tag ë§¤ì¹­ìš© í•¨ìˆ˜
def get_wordnet_pos(treebank_tag):
    if treebank_tag.startswith('J'):
        return wordnet.ADJ
    elif treebank_tag.startswith('V'):
        return wordnet.VERB
    elif treebank_tag.startswith('N'):
        return wordnet.NOUN
    elif treebank_tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN
    

#í’ˆì‚¬ íƒœê¹… + í‘œì œì–´ ì¶”ì¶œ
def tockenize(text):
    tokens=word_tokenize(text)
    pos_tokens=nltk.pos_tag(tokens)
    
    text_t=list()
    for _ in pos_tokens:
        text_t.append([_[0], get_wordnet_pos(_[1])])
    
    lemmatizer = WordNetLemmatizer()
    text = ' '.join([lemmatizer.lemmatize(word[0], word[1]) for word in text_t])
    return text

def clean(text):
    text = remove_special_characters(text, remove_digits=True)
    text = delete_stops(text)
    text = tockenize(text)
    return text


translator = Translator()
for col in ['ì±…ì†Œê°œ', 'ì±…ì†ìœ¼ë¡œ', 'ì„œí‰']:
    name = col+'_trans'
    if book[col].values == '':
        book[name] = ''
        continue
    book[name] = clean(translator.translate(hapus_url(book.loc[0, col])).text)

total_text = book.loc[0, 'ì±…ì†Œê°œ_trans'] + book.loc[0, 'ì±…ì†ìœ¼ë¡œ_trans'] + book.loc[0, 'ì„œí‰_trans']

df = pd.read_csv('tweet_data_agumentation.csv', index_col = 0)

tfidf_vect_emo = TfidfVectorizer()
tfidf_vect_emo.fit_transform(df["content"])

model = joblib.load('SVM.pkl')
total_text2 = tfidf_vect_emo.transform(pd.Series(total_text))
model.predict_proba(total_text2)
sentiment = pd.DataFrame(model.predict_proba(total_text2),index=['prob']).T
sentiment['ê°ì •'] = ['empty','sadness','enthusiasm','worry','love','fun','hate','happiness','boredom','relief','anger']
sentiment2 = sentiment.sort_values(by='prob',ascending=False)

my_bar.progress(60, text='ã€°ï¸60%ã€°ï¸')

# audio featureë‘ text ê°ì •
audio_data = data.iloc[:,-12:-1]

sentiment_prob = sentiment['prob']
sentiment_prob.index = sentiment['ê°ì •']

audio_data.columns = ['empty', 'sadness', 'enthusiasm', 'worry', 'love', 'fun', 'hate',
       'happiness', 'boredom', 'relief', 'anger']

audio_data_1 = pd.concat([sentiment_prob,audio_data.T],axis=1).T

col = ['book']+list(data['name'])
cosine_sim_audio = cosine_similarity(audio_data_1)
cosine_sim_audio_df = pd.DataFrame(cosine_sim_audio, index = col, columns=col)

audio_sim = cosine_sim_audio_df['book']

# ê°€ì‚¬ë‘ text
lyrics_data = data.iloc[:,5:-12]
lyrics_data_1 = pd.concat([sentiment_prob,lyrics_data.T],axis=1).T
cosine_sim_lyrics = cosine_similarity(lyrics_data_1)
cosine_sim_lyrics_df = pd.DataFrame(cosine_sim_lyrics, index =col, columns=col)
lyrics_sim = cosine_sim_lyrics_df['book']
my_bar.progress(80, text='ã€°ï¸80%ã€°ï¸')

# í‚¤ì›Œë“œë‘ text
keyword_data = data['key_word']
book_song_cont1 = pd.DataFrame({"text": total_text}, index = range(1))
book_song_cont2 = pd.DataFrame({"text": keyword_data})
keyword_data_1 = pd.concat([book_song_cont1, book_song_cont2], axis=0).reset_index(drop=True)

tfidf_vect_cont = TfidfVectorizer()
tfidf_matrix_cont = tfidf_vect_cont.fit_transform(keyword_data_1['text'])
tfidf_array_cont = tfidf_matrix_cont.toarray()
tfidf_df_cont = pd.DataFrame(tfidf_array_cont, columns=tfidf_vect_cont.get_feature_names_out())

cosine_sim_keyword = cosine_similarity(tfidf_array_cont)
cosine_sim_keyword_df = pd.DataFrame(cosine_sim_keyword, index = col, columns=col)
keyword_sim = cosine_sim_keyword_df['book']

# ì „ì²´ ìœ ì‚¬ë„ ê³„ì‚°
total_sim  = 0.8*audio_sim + 0.1*lyrics_sim + 0.1*keyword_sim

recommend_song = total_sim.sort_values(ascending=False)[1:6].index
total_sim_df = pd.DataFrame(total_sim[1:])
total_sim_df = total_sim_df.reset_index()
total_sim_df.columns = ['name','book']

top_five = total_sim_df.sort_values(by='book',ascending=False)[:5]
index = total_sim_df.sort_values(by='book',ascending=False)[:5].index.sort_values()
artist = data.iloc[index][['url','name','Artist']]
top_five_df = pd.merge(artist,top_five,on='name').sort_values(by='book',ascending=False).drop_duplicates()

total_sim  = 0*audio_sim + 0.5*lyrics_sim + 0.5*keyword_sim

recommend_song = total_sim.sort_values(ascending=False)[1:6].index
total_sim_df_1 = pd.DataFrame(total_sim[1:])
total_sim_df_1 = total_sim_df_1.reset_index()
total_sim_df_1.columns = ['name','book']

top_five_1 = total_sim_df_1.sort_values(by='book',ascending=False)[:5]
index_1 = total_sim_df_1.sort_values(by='book',ascending=False)[:5].index.sort_values()

artist = data.iloc[index_1][['url','name','Artist']]
top_five_df_1 = pd.merge(artist,top_five_1,on='name').sort_values(by='book',ascending=False).drop_duplicates()


my_bar.progress(100, text='100%')
time.sleep(1)
my_bar.empty()

st.markdown('')

st.header('2ï¸âƒ£ ê²°ê³¼')
st.subheader('ğŸ™‚ ë…¸ë˜ì™€ ë¶„ìœ„ê¸°ê°€ ìœ ì‚¬í•œ ë…¸ë˜')
st.caption('AF : ê°€ì‚¬ : í‚¤ì›Œë“œ = 0.8 : 0.1 : 0.1')
tab1, tab2, tab3, tab4, tab5= st.tabs(['TOP 1' , 'TOP 2', 'TOP 3', 'TOP 4', 'TOP 5'])
with tab1:
    st.subheader('ğŸ¥‡ TOP 1')
    st.markdown('**ì œëª©** : {0}'.format(top_five_df.iloc[0]['name']))
    st.markdown('**ê°€ìˆ˜** : {0} '.format(top_five_df.iloc[0]['Artist']))
    st.markdown('**url** : {0} '.format(top_five_df.iloc[0]['url']))
    st.markdown('**ìœ ì‚¬ë„** : {0:.4f}'.format(top_five_df.iloc[0]['book']))
    st.markdown('')
with tab2:
    st.subheader('ğŸ¥ˆ TOP 2')
    st.markdown('**ì œëª©** : {0}'.format(top_five_df.iloc[1]['name']))
    st.markdown('**ê°€ìˆ˜** : {0} '.format(top_five_df.iloc[1]['Artist']))
    st.markdown('**url** : {0} '.format(top_five_df.iloc[1]['url']))
    st.markdown('**ìœ ì‚¬ë„** : {0:.4f}'.format(top_five_df.iloc[1]['book']))
    st.markdown('')
with tab3:
    st.subheader('ğŸ¥‰ TOP 3')
    st.markdown('**ì œëª©** : {0}'.format(top_five_df.iloc[2]['name']))
    st.markdown('**ê°€ìˆ˜** : {0} '.format(top_five_df.iloc[2]['Artist']))
    st.markdown('**url** : {0} '.format(top_five_df.iloc[2]['url']))
    st.markdown('**ìœ ì‚¬ë„** : {0:.4f}'.format(top_five_df.iloc[2]['book']))
    st.markdown('')
with tab4:
    st.subheader('TOP 4')
    st.markdown('**ì œëª©** : {0}'.format(top_five_df.iloc[3]['name']))
    st.markdown('**ê°€ìˆ˜** : {0} '.format(top_five_df.iloc[3]['Artist']))
    st.markdown('**url** : {0} '.format(top_five_df.iloc[3]['url']))
    st.markdown('**ìœ ì‚¬ë„** : {0:.4f}'.format(top_five_df.iloc[3]['book']))
    st.markdown('')
with tab5:
    st.subheader('TOP 5')
    st.markdown('**ì œëª©** : {0}'.format(top_five_df.iloc[4]['name']))
    st.markdown('**ê°€ìˆ˜** : {0} '.format(top_five_df.iloc[4]['Artist']))
    st.markdown('**url** : {0} '.format(top_five_df.iloc[4]['url']))
    st.markdown('**ìœ ì‚¬ë„** : {0:.4f}'.format(top_five_df.iloc[4]['book']))

st.subheader('ğŸ“– ë…¸ë˜ì™€ ë‚´ìš©ì´ ìœ ì‚¬í•œ ë…¸ë˜')
st.caption('AF : ê°€ì‚¬ : í‚¤ì›Œë“œ = 0 : 0.5 : 0.5')
tab1, tab2, tab3, tab4, tab5= st.tabs(['TOP 1' , 'TOP 2', 'TOP 3', 'TOP 4', 'TOP 5'])
with tab1:
    st.subheader('ğŸ¥‡ TOP 1')
    st.markdown('**ì œëª©** : {0}'.format(top_five_df_1.iloc[0]['name']))
    st.markdown('**ê°€ìˆ˜** : {0} '.format(top_five_df_1.iloc[0]['Artist']))
    st.markdown('**url** : {0} '.format(top_five_df_1.iloc[0]['url']))
    st.markdown('**ìœ ì‚¬ë„** : {0:.4f}'.format(top_five_df_1.iloc[0]['book']))
    st.markdown('')
with tab2:
    st.subheader('ğŸ¥ˆ TOP 2')
    st.markdown('**ì œëª©** : {0}'.format(top_five_df_1.iloc[1]['name']))
    st.markdown('**ê°€ìˆ˜** : {0} '.format(top_five_df_1.iloc[1]['Artist']))
    st.markdown('**url** : {0} '.format(top_five_df_1.iloc[1]['url']))
    st.markdown('**ìœ ì‚¬ë„** : {0:.4f}'.format(top_five_df_1.iloc[1]['book']))
    st.markdown('')
with tab3:
    st.subheader('ğŸ¥‰ TOP 3')
    st.markdown('**ì œëª©** : {0}'.format(top_five_df_1.iloc[2]['name']))
    st.markdown('**ê°€ìˆ˜** : {0} '.format(top_five_df_1.iloc[2]['Artist']))
    st.markdown('**url** : {0} '.format(top_five_df_1.iloc[2]['url']))
    st.markdown('**ìœ ì‚¬ë„** : {0:.4f}'.format(top_five_df_1.iloc[2]['book']))
    st.markdown('')
with tab4:
    st.subheader('TOP 4')
    st.markdown('**ì œëª©** : {0}'.format(top_five_df_1.iloc[3]['name']))
    st.markdown('**ê°€ìˆ˜** : {0} '.format(top_five_df_1.iloc[3]['Artist']))
    st.markdown('**url** : {0} '.format(top_five_df_1.iloc[3]['url']))
    st.markdown('**ìœ ì‚¬ë„** : {0:.4f}'.format(top_five_df_1.iloc[3]['book']))
    st.markdown('')
with tab5:
    st.subheader('TOP 5')
    st.markdown('**ì œëª©** : {0}'.format(top_five_df_1.iloc[4]['name']))
    st.markdown('**ê°€ìˆ˜** : {0} '.format(top_five_df_1.iloc[4]['Artist']))
    st.markdown('**url** : {0} '.format(top_five_df_1.iloc[4]['url']))
    st.markdown('**ìœ ì‚¬ë„** : {0:.4f}'.format(top_five_df_1.iloc[4]['book']))